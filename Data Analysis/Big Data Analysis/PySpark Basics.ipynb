
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ef693a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\mohanraj\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in c:\\users\\mohanraj\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea77340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# For Starting a New Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cb21c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-LTPC6DQE:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f660afe340>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78557edd",
   "metadata": {},
   "source": [
    "# PySpark DataFrame\n",
    "# Reading the Dataset\n",
    "# Checking the DataType of the Column\n",
    "# Selecting Columns and Indexing\n",
    "# Check Describe Option Similar to Pandas \n",
    "# Adding, Dropping and Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33b50edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+\n",
      "|_c0|Score|      Category|\n",
      "+---+-----+--------------+\n",
      "| 15|53935|        travel|\n",
      "| 10|53657|       science|\n",
      "|  8|52745|healthy eating|\n",
      "|  1|52443|       animals|\n",
      "|  2|49681|       cooking|\n",
      "+---+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Reading the data\n",
    "df_Spark = spark.read.csv(r\"H:\\Mohan\\Downloads\\Top5Category.csv\", header=True , inferSchema=True) # By default spark read all column as string datatype but by adding inferschema = True it can get the actual datatype\n",
    "df_Spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1aac8d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Score: integer (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO Find the Datatype of the dataframe similar to pandas (info)\n",
    "df_Spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6841d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      Category|Score|\n",
      "+--------------+-----+\n",
      "|        travel|53935|\n",
      "|       science|53657|\n",
      "|healthy eating|52745|\n",
      "|       animals|52443|\n",
      "|       cooking|49681|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to select a single column\n",
    "specific_column = df_Spark.select([\"Category\",\"Score\"])\n",
    "specific_column.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0c9cf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+\n",
      "|summary|Category|             Score|\n",
      "+-------+--------+------------------+\n",
      "|  count|       5|                 5|\n",
      "|   mean|    null|           52492.2|\n",
      "| stddev|    null|1688.7572945808402|\n",
      "|    min| animals|             49681|\n",
      "|    max|  travel|             53935|\n",
      "+-------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe Options\n",
    "specific_column.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "428ab11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+--------------+\n",
      "|_c0|Score|      Category|Score With Tax|\n",
      "+---+-----+--------------+--------------+\n",
      "| 15|53935|        travel|         54935|\n",
      "| 10|53657|       science|         54657|\n",
      "|  8|52745|healthy eating|         53745|\n",
      "|  1|52443|       animals|         53443|\n",
      "|  2|49681|       cooking|         50681|\n",
      "+---+-----+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding column in PySpark DataFrame\n",
    "\n",
    "df_Spark=df_Spark.withColumn(\"Score With Tax\",df_Spark[\"Score\"]+1000)\n",
    "df_Spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f7352fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+\n",
      "|_c0|Score|      Category|\n",
      "+---+-----+--------------+\n",
      "| 15|53935|        travel|\n",
      "| 10|53657|       science|\n",
      "|  8|52745|healthy eating|\n",
      "|  1|52443|       animals|\n",
      "|  2|49681|       cooking|\n",
      "+---+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping the Column in PySpark DataFrame\n",
    "df_Spark = df_Spark.drop(\"Score With Tax\")\n",
    "df_Spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "edd51833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------+\n",
      "|Rank|Score|      Category|\n",
      "+----+-----+--------------+\n",
      "|  15|53935|        travel|\n",
      "|  10|53657|       science|\n",
      "|   8|52745|healthy eating|\n",
      "|   1|52443|       animals|\n",
      "|   2|49681|       cooking|\n",
      "+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming the Columns\n",
    "df_Spark = df_Spark.withColumnRenamed('_c0','Rank')\n",
    "df_Spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e2162",
   "metadata": {},
   "source": [
    "# Dropping Rows\n",
    "# Various Parameter in Dropping Function\n",
    "# Handling Missing Values by Means, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8917915e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------------------+--------------+------+\n",
      "| Id|  Name| Age|        Profession|          City|Salary|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "|100| Kumar|  27|            Lawyer|     Riverside|  1558|\n",
      "|101|  null|  39|          Musician|        Maleee|  null|\n",
      "|102|Kurnal|  48|            Police|  Bahia blanca|  7612|\n",
      "|103|Monesh|null|           Teacher|          null|  2451|\n",
      "|104|   Raj|  51|Software Developer|     Amristsar|  3599|\n",
      "|105| Mohan|  36|            Doctor|      Montreal|  null|\n",
      "|106| Anish|  55|            Police|Sanfrancesisco|  3578|\n",
      "|107|  null|  36|            Police|          Gaza|  2268|\n",
      "|108| Priya|null|           Student|       Hamburg|  3971|\n",
      "|109| Ramya|  24|          Designer|          null|  9964|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Reading the data\n",
    "df_Spark_1 = spark.read.csv(r\"C:\\Users\\MOHANRAJ\\Desktop\\Data streaming analysis\\StreamData.csv\", header=True , inferSchema=True) # By default spark read all column as string datatype but by adding inferschema = True it can get the actual datatype\n",
    "df_Spark_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cee7d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Spark_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47f1a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------------+--------------+------+\n",
      "| Id|  Name|Age|        Profession|          City|Salary|\n",
      "+---+------+---+------------------+--------------+------+\n",
      "|100| Kumar| 27|            Lawyer|     Riverside|  1558|\n",
      "|102|Kurnal| 48|            Police|  Bahia blanca|  7612|\n",
      "|104|   Raj| 51|Software Developer|     Amristsar|  3599|\n",
      "|106| Anish| 55|            Police|Sanfrancesisco|  3578|\n",
      "+---+------+---+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping Null Values\n",
    "df_Spark_1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "89b8e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------------------+--------------+------+\n",
      "| Id|  Name| Age|        Profession|          City|Salary|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "|100| Kumar|  27|            Lawyer|     Riverside|  1558|\n",
      "|102|Kurnal|  48|            Police|  Bahia blanca|  7612|\n",
      "|104|   Raj|  51|Software Developer|     Amristsar|  3599|\n",
      "|105| Mohan|  36|            Doctor|      Montreal|  null|\n",
      "|106| Anish|  55|            Police|Sanfrancesisco|  3578|\n",
      "|107|  null|  36|            Police|          Gaza|  2268|\n",
      "|108| Priya|null|           Student|       Hamburg|  3971|\n",
      "|109| Ramya|  24|          Designer|          null|  9964|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping ParaMeter how , thresh and subset\n",
    "df_Spark_1.na.drop(how = \"any\", thresh=5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "670a3071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------------------+--------------+------+\n",
      "| Id|  Name| Age|        Profession|          City|Salary|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "|100| Kumar|  27|            Lawyer|     Riverside|  1558|\n",
      "|102|Kurnal|  48|            Police|  Bahia blanca|  7612|\n",
      "|103|Monesh|null|           Teacher|          null|  2451|\n",
      "|104|   Raj|  51|Software Developer|     Amristsar|  3599|\n",
      "|106| Anish|  55|            Police|Sanfrancesisco|  3578|\n",
      "|107|  null|  36|            Police|          Gaza|  2268|\n",
      "|108| Priya|null|           Student|       Hamburg|  3971|\n",
      "|109| Ramya|  24|          Designer|          null|  9964|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropping null values in a particular column we can use SubSet parameter \n",
    "df_Spark_1.na.drop(how = \"any\", subset=[\"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b12dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+------------------+--------------+------+\n",
      "| Id|  Name| Age|        Profession|          City|Salary|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "|100| Kumar|  27|            Lawyer|     Riverside|  1558|\n",
      "|101|   NAN|  39|          Musician|        Maleee|  null|\n",
      "|102|Kurnal|  48|            Police|  Bahia blanca|  7612|\n",
      "|103|Monesh|null|           Teacher|           NAN|  2451|\n",
      "|104|   Raj|  51|Software Developer|     Amristsar|  3599|\n",
      "|105| Mohan|  36|            Doctor|      Montreal|  null|\n",
      "|106| Anish|  55|            Police|Sanfrancesisco|  3578|\n",
      "|107|   NAN|  36|            Police|          Gaza|  2268|\n",
      "|108| Priya|null|           Student|       Hamburg|  3971|\n",
      "|109| Ramya|  24|          Designer|           NAN|  9964|\n",
      "+---+------+----+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling Null value\n",
    "df_Spark_1.na.fill(\"NAN\",['Name','City']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "95bcd247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------------+--------------+------+\n",
      "| Id|  Name|Age|        Profession|          City|Salary|\n",
      "+---+------+---+------------------+--------------+------+\n",
      "|100| Kumar| 27|            Lawyer|     Riverside|  1558|\n",
      "|101|  null| 39|          Musician|        Maleee|  4375|\n",
      "|102|Kurnal| 48|            Police|  Bahia blanca|  7612|\n",
      "|103|Monesh| 39|           Teacher|          null|  2451|\n",
      "|104|   Raj| 51|Software Developer|     Amristsar|  3599|\n",
      "|105| Mohan| 36|            Doctor|      Montreal|  4375|\n",
      "|106| Anish| 55|            Police|Sanfrancesisco|  3578|\n",
      "|107|  null| 36|            Police|          Gaza|  2268|\n",
      "|108| Priya| 39|           Student|       Hamburg|  3971|\n",
      "|109| Ramya| 24|          Designer|          null|  9964|\n",
      "+---+------+---+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing Imputer for filling na values with mean median and mode\n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "                    inputCols = ['Age','Salary'],\n",
    "                    outputCols = [c for c in ['Age','Salary']]\n",
    "                    ).setStrategy(\"mean\") # median and Mode\n",
    "\n",
    "# Fiting and Transform imputer in df_spark_1\n",
    "df_Spark_1 = imputer.fit(df_Spark_1).transform(df_Spark_1)\n",
    "df_Spark_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f5c280",
   "metadata": {},
   "source": [
    "# Filter Operation\n",
    "# Operators\n",
    "# ~ inverse filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4718c675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------------+--------------+------+\n",
      "| Id|  Name|Age|        Profession|          City|Salary|\n",
      "+---+------+---+------------------+--------------+------+\n",
      "|100| Kumar| 27|            Lawyer|     Riverside|  1558|\n",
      "|101| Priya| 39|          Musician|        Maleee|  2268|\n",
      "|102|Kurnal| 48|            Police|  Bahia blanca|  7612|\n",
      "|103|Monesh| 36|           Teacher|     Riverside|  2451|\n",
      "|104|   Raj| 51|Software Developer|     Amristsar|  3599|\n",
      "|105| Mohan| 36|            Doctor|      Montreal|  2268|\n",
      "|106| Anish| 55|            Police|Sanfrancesisco|  3578|\n",
      "|107| Priya| 36|            Police|          Gaza|  2268|\n",
      "|108| Priya| 36|           Student|       Hamburg|  3971|\n",
      "|109| Ramya| 24|          Designer|     Riverside|  9964|\n",
      "+---+------+---+------------------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_2  = spark.read.csv(r\"C:\\Users\\MOHANRAJ\\Desktop\\Data streaming analysis\\StreamData.csv\",header =True, inferSchema= True)\n",
    "df_spark_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3611e954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+----------+------------+------+\n",
      "| Id|  Name|Age|Profession|        City|Salary|\n",
      "+---+------+---+----------+------------+------+\n",
      "|102|Kurnal| 48|    Police|Bahia blanca|  7612|\n",
      "|109| Ramya| 24|  Designer|   Riverside|  9964|\n",
      "+---+------+---+----------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Operation\n",
    "\n",
    "# Salary greater than 5000\n",
    "df_spark_2.filter(\"Salary>5000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32ce7444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|  Name|Profession|Salary|\n",
      "+------+----------+------+\n",
      "|Kurnal|    Police|  7612|\n",
      "| Ramya|  Designer|  9964|\n",
      "+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_2.filter(\"Salary>5000\").select(['Name','Profession','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868cc62",
   "metadata": {},
   "source": [
    "# Groupby and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8a5a534a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+------------+---------+------+\n",
      "| Id|  Name|Age|  Profession|     City|Salary|\n",
      "+---+------+---+------------+---------+------+\n",
      "|100| Kumar| 27|Data Science|  Chennai|  1558|\n",
      "|101| Priya| 39|Data Analyst|Bangalore|  2268|\n",
      "|102|Kurnal| 48|Data Analyst|Bangalore|  7612|\n",
      "|103|Monesh| 36|Bi Developer|  Chennai|  2451|\n",
      "|104|   Raj| 51|Data Science|Bangalore|  3599|\n",
      "|105| Mohan| 36|Bi Developer|  Chennai|  2268|\n",
      "|106| Anish| 55|Bi Developer|Bangalore|  3578|\n",
      "|107| Priya| 36|Data Analyst|    Delhi|  2268|\n",
      "|108| Priya| 36|Data Science|    Delhi|  3971|\n",
      "|109| Ramya| 24|Data Analyst|  Chennai|  9964|\n",
      "+---+------+---+------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_3  = spark.read.csv(r\"C:\\Users\\MOHANRAJ\\Desktop\\Data streaming analysis\\StreamData.csv\",header =True, inferSchema= True)\n",
    "df_spark_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d6d758d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------+-----------+\n",
      "|  Profession|sum(Id)|sum(Age)|sum(Salary)|\n",
      "+------------+-------+--------+-----------+\n",
      "|Bi Developer|    314|     127|       8297|\n",
      "|Data Analyst|    419|     147|      22112|\n",
      "|Data Science|    312|     114|       9128|\n",
      "+------------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby\n",
    "df_spark_3.groupBy(\"Profession\").sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa6851a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------+-----------+\n",
      "|     City|avg(Id)|avg(Age)|avg(Salary)|\n",
      "+---------+-------+--------+-----------+\n",
      "|Bangalore| 103.25|   48.25|    4264.25|\n",
      "|  Chennai| 104.25|   30.75|    4060.25|\n",
      "|    Delhi|  107.5|    36.0|     3119.5|\n",
      "+---------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_3.groupBy(\"City\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a783362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|  Profession|count|\n",
      "+------------+-----+\n",
      "|Bi Developer|    3|\n",
      "|Data Analyst|    4|\n",
      "|Data Science|    3|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_3.groupBy(\"Profession\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b1e0c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------+-----------+\n",
      "|  Profession|max(Id)|max(Age)|max(Salary)|\n",
      "+------------+-------+--------+-----------+\n",
      "|Bi Developer|    106|      55|       3578|\n",
      "|Data Analyst|    109|      48|       9964|\n",
      "|Data Science|    108|      51|       3971|\n",
      "+------------+-------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_3.groupBy(\"Profession\").max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bae147a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      39537|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate Function\n",
    "\n",
    "df_spark_3.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18558f",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3aa3c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---+------------+---------+------+\n",
      "| Id|  Name|Experiance|Age|  Profession|     City|Salary|\n",
      "+---+------+----------+---+------------+---------+------+\n",
      "|100| Kumar|         5| 27|Data Science|  Chennai|  2000|\n",
      "|101| Priya|         7| 39|Data Analyst|Bangalore|  4000|\n",
      "|102|Kurnal|         2| 48|Data Analyst|Bangalore|  6000|\n",
      "|103|Monesh|         3| 36|Bi Developer|  Chennai|  8000|\n",
      "|104|   Raj|         5| 51|Data Science|Bangalore| 10000|\n",
      "|105| Mohan|        10| 36|Bi Developer|  Chennai| 12000|\n",
      "|106| Anish|         6| 55|Bi Developer|Bangalore| 14000|\n",
      "|107| Priya|         6| 36|Data Analyst|    Delhi| 16000|\n",
      "|108| Priya|         2| 36|Data Science|    Delhi| 18000|\n",
      "|109| Ramya|         1| 24|Data Analyst|  Chennai| 20000|\n",
      "+---+------+----------+---+------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_4  = spark.read.csv(r\"C:\\Users\\MOHANRAJ\\Desktop\\Data streaming analysis\\StreamData.csv\",header =True, inferSchema= True)\n",
    "df_spark_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "92fccda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Experiance: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Profession: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e29a5689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+---+------------+---------+------+-------------------+\n",
      "| Id|  Name|Experiance|Age|  Profession|     City|Salary|Independent feature|\n",
      "+---+------+----------+---+------------+---------+------+-------------------+\n",
      "|100| Kumar|         5| 27|Data Science|  Chennai|  2000|         [5.0,27.0]|\n",
      "|101| Priya|         7| 39|Data Analyst|Bangalore|  4000|         [7.0,39.0]|\n",
      "|102|Kurnal|         2| 48|Data Analyst|Bangalore|  6000|         [2.0,48.0]|\n",
      "|103|Monesh|         3| 36|Bi Developer|  Chennai|  8000|         [3.0,36.0]|\n",
      "|104|   Raj|         5| 51|Data Science|Bangalore| 10000|         [5.0,51.0]|\n",
      "|105| Mohan|        10| 36|Bi Developer|  Chennai| 12000|        [10.0,36.0]|\n",
      "|106| Anish|         6| 55|Bi Developer|Bangalore| 14000|         [6.0,55.0]|\n",
      "|107| Priya|         6| 36|Data Analyst|    Delhi| 16000|         [6.0,36.0]|\n",
      "|108| Priya|         2| 36|Data Science|    Delhi| 18000|         [2.0,36.0]|\n",
      "|109| Ramya|         1| 24|Data Analyst|  Chennai| 20000|         [1.0,24.0]|\n",
      "+---+------+----------+---+------------+---------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=['Experiance','Age'],outputCol='Independent feature')\n",
    "\n",
    "output = featureassembler.transform(df_spark_4)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c9cd5501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|Independent feature|Salary|\n",
      "+-------------------+------+\n",
      "|         [5.0,27.0]|  2000|\n",
      "|         [7.0,39.0]|  4000|\n",
      "|         [2.0,48.0]|  6000|\n",
      "|         [3.0,36.0]|  8000|\n",
      "|         [5.0,51.0]| 10000|\n",
      "|        [10.0,36.0]| 12000|\n",
      "|         [6.0,55.0]| 14000|\n",
      "|         [6.0,36.0]| 16000|\n",
      "|         [2.0,36.0]| 18000|\n",
      "|         [1.0,24.0]| 20000|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Final_Data=output.select('Independent feature','Salary')\n",
    "Final_Data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a9daf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data into train and test data\n",
    "train_data,test_data = Final_Data.randomSplit([0.70,0.30])\n",
    "\n",
    "# Linear Regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "Model = LinearRegression(featuresCol=\"Independent feature\",labelCol='Salary')\n",
    "Model = Model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0a283319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([117.6471, -588.2353])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Co efficient\n",
    "Model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8e437437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33058.823529411784"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercepts\n",
    "Model.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "703f7118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------------------+\n",
      "|Independent feature|Salary|        prediction|\n",
      "+-------------------+------+------------------+\n",
      "|         [2.0,36.0]| 18000|12117.647058823535|\n",
      "|         [5.0,27.0]|  2000|17764.705882352944|\n",
      "|         [5.0,51.0]| 10000| 3647.058823529409|\n",
      "|         [6.0,55.0]| 14000|1411.7647058823495|\n",
      "|         [7.0,39.0]|  4000|10941.176470588234|\n",
      "+-------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "prediction = Model.evaluate(test_data)\n",
    "prediction.predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
