{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf552c-c444-4e4a-af27-a030a7edf4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import sys\n",
    "\n",
    "def get_wikipedia_content(topic):\n",
    "    \"\"\"\n",
    "    Fetches the full Wikipedia content for a given topic.\n",
    "\n",
    "    Args:\n",
    "        topic (str): The title of the Wikipedia page to fetch.\n",
    "\n",
    "    Returns:\n",
    "        str or None: The full text content of the page if found, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = wikipedia.page(topic)\n",
    "        return page.content\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(\"Page not found. Please check the topic name.\")\n",
    "        return None\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Ambiguous topic. Please be more specific. Options: {e.options}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def split_text(text, tokenizer, chunk_size=256, chunk_overlap=20):\n",
    "    \"\"\"\n",
    "    Splits long text into overlapping token chunks for processing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full input text to be chunked.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer used for splitting.\n",
    "        chunk_size (int): Maximum number of tokens per chunk.\n",
    "        chunk_overlap (int): Number of tokens to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of chunked text segments.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk = tokenizer.convert_tokens_to_string(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - chunk_overlap  # Create overlap\n",
    "    return chunks\n",
    "\n",
    "def embed_chunks(chunks, model):\n",
    "    \"\"\"\n",
    "    Encodes a list of text chunks into vector embeddings.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): List of text chunks.\n",
    "        model (SentenceTransformer): Embedding model.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of vector embeddings.\n",
    "    \"\"\"\n",
    "    return model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "def build_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Builds a FAISS vector index for similarity search.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): Embeddings matrix.\n",
    "\n",
    "    Returns:\n",
    "        faiss.IndexFlatL2: FAISS index for fast nearest neighbor search.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]  # Dimension of embedding vectors\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
    "    index.add(embeddings)  # Add embeddings to the index\n",
    "    return index\n",
    "\n",
    "def retrieve_top_k_chunks(query, chunks, embedding_model, index, k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the top-k most relevant text chunks for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): User's question.\n",
    "        chunks (List[str]): Original text chunks.\n",
    "        embedding_model (SentenceTransformer): Embedding model for the query.\n",
    "        index (faiss.Index): FAISS index built from document chunks.\n",
    "        k (int): Number of top similar chunks to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of the most relevant text chunks.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(np.array(query_embedding), k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "def answer_question(question, context, qa_pipeline):\n",
    "    \"\"\"\n",
    "    Answers a question using the QA pipeline and the given context.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to answer.\n",
    "        context (str): Relevant context to extract answer from.\n",
    "        qa_pipeline (Pipeline): Hugging Face QA pipeline.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted answer.\n",
    "    \"\"\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "def main():\n",
    "    # Step 1: Get topic input from user\n",
    "    topic = input(\"Enter a topic to learn about: \")\n",
    "    document = get_wikipedia_content(topic)\n",
    "\n",
    "    # Exit if the topic is invalid\n",
    "    if not document:\n",
    "        print(\"Failed to retrieve content. Exiting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Step 2: Load tokenizer and embedding model for chunking and encoding\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    # Step 3: Split the document into overlapping chunks\n",
    "    chunks = split_text(document, tokenizer)\n",
    "    print(f\"[INFO] Text split into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Step 4: Embed the chunks into dense vectors\n",
    "    embeddings = embed_chunks(chunks, embedding_model)\n",
    "\n",
    "    # Step 5: Create a FAISS index for similarity search\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    # Step 6: Ask the user for a question about the topic\n",
    "    query = input(\"Ask a question about the topic: \")\n",
    "\n",
    "    # Step 7: Retrieve top-k relevant text chunks based on the query\n",
    "    retrieved_chunks = retrieve_top_k_chunks(query, chunks, embedding_model, index, k=3)\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "\n",
    "    # Step 8: Initialize QA model and tokenizer pipeline\n",
    "    qa_model_name = \"deepset/roberta-base-squad2\"\n",
    "    qa_pipeline_model = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=AutoModelForQuestionAnswering.from_pretrained(qa_model_name),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(qa_model_name)\n",
    "    )\n",
    "\n",
    "    # Step 9: Use the pipeline to get the final answer from context\n",
    "    answer = answer_question(query, context, qa_pipeline_model)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
