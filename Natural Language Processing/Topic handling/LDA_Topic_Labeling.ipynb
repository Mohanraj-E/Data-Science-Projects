{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d01f69e3",
   "metadata": {},
   "source": [
    "# #Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17c95160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Ids</th>\n",
       "      <th>Date</th>\n",
       "      <th>Flag</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target         Ids                          Date      Flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "          UserName                                               Text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Assigning the column names\n",
    "\n",
    "colnames=['Target', 'Ids', 'Date', 'Flag','UserName','Text'] \n",
    "Twitter_Data = pd.read_csv(r\"C:\\Users\\MOHANRAJ\\Desktop\\Projects\\GUVI Final Project\\NLP_LDA_Topic Handling\\twitter_new.csv\",encoding='latin-1', names=colnames, header=None)\n",
    "Twitter_Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc76f6",
   "metadata": {},
   "source": [
    "# #Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0601f094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    @switchfoot http://twitpiccom/2y1zl - awww tha...\n",
       "1    is upset that he can't update his facebook by ...\n",
       "2    @kenichan i dived many times for the ball mana...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4    @nationwideclass no it's not behaving at all i...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "Twitter_Data['Text'] = \\\n",
    "Twitter_Data['Text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "Twitter_Data['Text'] = \\\n",
    "Twitter_Data['Text'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "Twitter_Data['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f4dd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MOHANRAJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switchfoot', 'http', 'twitpiccom', 'zl', 'awww', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Removing Stopword and Tokenization\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "data = Twitter_Data.Text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5589685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Creating token using id2word and load it to the Corpus\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ec28c",
   "metadata": {},
   "source": [
    "# # LDA Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81cdda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.027*\"know\" + 0.024*\"really\" + 0.021*\"want\" + 0.017*\"lol\" + 0.012*\"think\" '\n",
      "  '+ 0.011*\"haha\" + 0.011*\"love\" + 0.011*\"im\" + 0.010*\"like\" + 0.010*\"dont\"'),\n",
      " (1,\n",
      "  '0.022*\"sleep\" + 0.019*\"please\" + 0.012*\"much\" + 0.011*\"get\" + 0.011*\"help\" '\n",
      "  '+ 0.010*\"need\" + 0.009*\"talk\" + 0.009*\"got\" + 0.009*\"good\" + 0.008*\"go\"'),\n",
      " (2,\n",
      "  '0.055*\"day\" + 0.027*\"good\" + 0.021*\"morning\" + 0.018*\"today\" + '\n",
      "  '0.015*\"thank\" + 0.015*\"happy\" + 0.014*\"night\" + 0.012*\"nice\" + '\n",
      "  '0.012*\"getting\" + 0.010*\"ready\"'),\n",
      " (3,\n",
      "  '0.070*\"quot\" + 0.035*\"http\" + 0.015*\"love\" + 0.013*\"bitly\" + '\n",
      "  '0.013*\"watching\" + 0.011*\"movie\" + 0.010*\"hi\" + 0.007*\"say\" + 0.007*\"new\" + '\n",
      "  '0.007*\"one\"'),\n",
      " (4,\n",
      "  '0.032*\"see\" + 0.021*\"love\" + 0.018*\"back\" + 0.016*\"get\" + 0.014*\"come\" + '\n",
      "  '0.012*\"go\" + 0.010*\"never\" + 0.010*\"wait\" + 0.008*\"soon\" + 0.008*\"yes\"'),\n",
      " (5,\n",
      "  '0.044*\"http\" + 0.020*\"twitpiccom\" + 0.011*\"welcome\" + 0.009*\"good\" + '\n",
      "  '0.008*\"wish\" + 0.008*\"dinner\" + 0.007*\"coffee\" + 0.007*\"time\" + '\n",
      "  '0.007*\"friends\" + 0.007*\"rain\"'),\n",
      " (6,\n",
      "  '0.021*\"going\" + 0.019*\"fun\" + 0.013*\"got\" + 0.012*\"night\" + 0.011*\"well\" + '\n",
      "  '0.009*\"great\" + 0.008*\"back\" + 0.008*\"time\" + 0.008*\"miss\" + 0.007*\"home\"'),\n",
      " (7,\n",
      "  '0.033*\"like\" + 0.023*\"thanks\" + 0.014*\"twitter\" + 0.010*\"cool\" + '\n",
      "  '0.010*\"lol\" + 0.010*\"follow\" + 0.009*\"one\" + 0.009*\"good\" + 0.007*\"love\" + '\n",
      "  '0.007*\"amp\"'),\n",
      " (8,\n",
      "  '0.034*\"work\" + 0.025*\"go\" + 0.017*\"days\" + 0.017*\"tomorrow\" + 0.017*\"time\" '\n",
      "  '+ 0.012*\"get\" + 0.010*\"back\" + 0.009*\"home\" + 0.009*\"im\" + 0.009*\"got\"'),\n",
      " (9,\n",
      "  '0.029*\"good\" + 0.022*\"new\" + 0.017*\"lt\" + 0.012*\"hey\" + 0.011*\"im\" + '\n",
      "  '0.008*\"get\" + 0.008*\"gt\" + 0.008*\"luck\" + 0.007*\"lol\" + 0.007*\"nothing\"')]\n"
     ]
    }
   ],
   "source": [
    "# Bulding LDA_Model pprint the Revalent Topics\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
